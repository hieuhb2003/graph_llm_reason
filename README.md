# Element Mapping Between Knowledge Graphs

This script (`entity_mapping.py`) maps entities and edges between two knowledge graphs based on the similarity of their descriptions stored in Vector Database (VDB) files generated by LightRAG. It identifies pairs of VDB entries (one from each graph) whose description embeddings are highly similar and saves the details of these pairs to a JSON file.

This mapping file can then be used to find corresponding entities or edges across graphs, which is useful for tasks like cross-lingual information retrieval.

## Prerequisites

1.  **Python Environment:** Python 3.8+ recommended.
2.  **Libraries:**
    - `lightrag`: Ensure the LightRAG library (including `kg.nano_vector_db_impl` and `utils`) is installed and accessible in your Python environment.
    - `numpy`: Required by LightRAG components.
3.  **Vector Database (VDB) Files:** You need four VDB JSON files generated by LightRAG's indexing process (e.g., using `lightrag.LightRAG.a_create_db_with_new_embedding`):
    - Entity VDB for Graph 1 (e.g., `vdb_VECTOR_STORE_ENTITIES_en_embedding_name.json`)
    - Entity VDB for Graph 2 (e.g., `vdb_VECTOR_STORE_ENTITIES_vi_embedding_name.json`)
    - Edge VDB for Graph 1 (e.g., `vdb_VECTOR_STORE_RELATIONSHIPS_en_embedding_name.json`)
    - Edge VDB for Graph 2 (e.g., `vdb_VECTOR_STORE_RELATIONSHIPS_vi_embedding_name.json`)
    - **Crucially**, each entry within the `"data"` list of these VDB files _must_ contain the necessary keys used for mapping:
      - For entities: `"entity_name"`, `"chunk_id"`, `"description"`.
      - For edges: `"src_id"`, `"tgt_id"`, `"chunk_id"`, `"description"`.

## How to Run

Execute the script from your terminal:

```bash
python entity_mapping.py \
    --graph1_name <name_g1> \
    --graph2_name <name_g2> \
    --g1_entity_vdb <path_to_g1_entity_vdb.json> \
    --g2_entity_vdb <path_to_g2_entity_vdb.json> \
    --g1_edge_vdb <path_to_g1_edge_vdb.json> \
    --g2_edge_vdb <path_to_g2_edge_vdb.json> \
    --output <output_directory> \
    --threshold <similarity_threshold> \
    --embedding_dim <vdb_embedding_dimension>
```

**Arguments:**

- `--graph1_name`: A short name for the first graph (e.g., `en`, `graph_a`). Used in the output filename.
- `--graph2_name`: A short name for the second graph (e.g., `vi`, `graph_b`). Used in the output filename.
- `--g1_entity_vdb`: Full path to the entity VDB JSON file for graph 1.
- `--g2_entity_vdb`: Full path to the entity VDB JSON file for graph 2.
- `--g1_edge_vdb`: Full path to the edge VDB JSON file for graph 1.
- `--g2_edge_vdb`: Full path to the edge VDB JSON file for graph 2.
- `--output`: Path to the directory where the output JSON mapping file will be saved.
- `--threshold` (optional): The cosine similarity threshold (between 0.0 and 1.0) for considering two VDB entries as a match. Defaults to `0.8`. Higher values mean stricter matching.
- `--embedding_dim` (optional): The dimension of the embeddings stored in the VDB files. Defaults to `1024`. This must match the dimension used when creating the VDBs.

**Example:**

```bash
python entity_mapping.py \
    --graph1_name en \
    --graph2_name vi \
    --g1_entity_vdb ./lightrag_cache_en/vdb_VECTOR_STORE_ENTITIES_my_embedding.json \
    --g2_entity_vdb ./lightrag_cache_vi/vdb_VECTOR_STORE_ENTITIES_my_embedding.json \
    --g1_edge_vdb ./lightrag_cache_en/vdb_VECTOR_STORE_RELATIONSHIPS_my_embedding.json \
    --g2_edge_vdb ./lightrag_cache_vi/vdb_VECTOR_STORE_RELATIONSHIPS_my_embedding.json \
    --output ./mapping_results \
    --threshold 0.85 \
    --embedding_dim 768
```

## Output File

The script generates a single JSON file in the specified output directory named like `element_mapping_<graph1_name>_<graph2_name>_thresh<threshold>.json`.

This file contains a JSON array (list) where each element is a dictionary representing a matched pair. Each dictionary has the following structure:

**For Entity Matches:**

```json
{
  "type": "entity",
  "similarity_score": 0.85123,
  "graph1_entity_name": "Name in Graph 1",
  "graph1_trigger_chunk_id": "chunk_id_from_g1_vdb",
  "graph1_trigger_description": "Specific description from g1 VDB entry",
  "graph2_entity_name": "Corresponding Name in Graph 2",
  "graph2_trigger_chunk_id": "chunk_id_from_g2_vdb",
  "graph2_trigger_description": "Specific description from g2 VDB entry"
}
```

**For Edge Matches:**

```json
{
  "type": "edge",
  "similarity_score": 0.91234,
  "graph1_src_entity": "Source Name in Graph 1",
  "graph1_tgt_entity": "Target Name in Graph 1",
  "graph1_trigger_chunk_id": "chunk_id_from_g1_vdb",
  "graph1_trigger_description": "Specific description from g1 VDB entry",
  "graph2_src_entity": "Corresponding Source Name in Graph 2",
  "graph2_tgt_entity": "Corresponding Target Name in Graph 2",
  "graph2_trigger_chunk_id": "chunk_id_from_g2_vdb",
  "graph2_trigger_description": "Specific description from g2 VDB entry"
}
```

The list is sorted by `similarity_score` in descending order.

## Using the Mapping File

The generated JSON file can be loaded and queried to find corresponding elements between the graphs. The `entity_mapping.py` script includes two utility functions for this purpose:

- `find_mapped_entity_description(mapping_file_path, input_entity_name, input_description, source_graph_key_prefix, target_graph_key_prefix)`: Finds the mapped entity name and description in the target graph given the name and _triggering description_ from the source graph.
- `find_mapped_edge_description(mapping_file_path, input_src_entity, input_tgt_entity, input_description, source_graph_key_prefix, target_graph_key_prefix)`: Finds the mapped source entity, target entity, and description in the target graph given the source/target names and _triggering description_ from the source graph.

These functions perform an exact match on the provided description.

```python
# Example using the utility functions
from entity_mapping import find_mapped_entity_description, find_mapped_edge_description

mapping_file = "./mapping_results/element_mapping_en_vi_thresh0.85.json"

# Find entity mapping from en (graph1) to vi (graph2)
entity_result = find_mapped_entity_description(
    mapping_file,
    '"DOG"', # Entity name from graph 1
    "A furry animal.", # Specific VDB description from graph 1
    source_graph_key_prefix="graph1",
    target_graph_key_prefix="graph2"
)
if entity_result:
    mapped_name, mapped_desc = entity_result
    print(f"Mapped Entity: {mapped_name}, Desc: {mapped_desc}")

# Find edge mapping from en (graph1) to vi (graph2)
edge_result = find_mapped_edge_description(
    mapping_file,
    '"DOG"', # Source entity from graph 1
    '"CAT"', # Target entity from graph 1
    "Plays with.", # Specific VDB edge description from graph 1
    source_graph_key_prefix="graph1",
    target_graph_key_prefix="graph2"
)
if edge_result:
    mapped_src, mapped_tgt, mapped_desc = edge_result
    print(f"Mapped Edge: {mapped_src} -> {mapped_tgt}, Desc: {mapped_desc}")

```

# PageRank-Based Retrieval

NanoGraphRAG now supports PageRank-based retrieval, which leverages the knowledge graph structure to enhance retrieval quality. This approach combines traditional vector similarity with graph-based reasoning.

## How PageRank Retrieval Works

1. **Graph-enhanced retrieval:** Instead of using only vector similarity, PageRank retrieval uses the knowledge graph to find connected information relevant to your query.

2. **Core algorithm:** The approach is based on Personalized PageRank, which biases the graph exploration toward:

   - Entities that are semantically similar to the query
   - Relationships (facts) that are relevant to the query
   - Documents that have good vector similarity with the query

3. **Advantages:**
   - Better handling of complex queries requiring multi-hop reasoning
   - Improved retrieval of facts that are connected but not explicitly mentioned
   - Greater contextual awareness through the graph structure

## Performance Optimization with Precomputation

To improve performance, NanoGraphRAG supports precomputation of expensive components:

1. **Precompute once:** Calculate embeddings for all graph edges and passages just once
2. **Reuse for multiple queries:** Only compute query embeddings for each new query
3. **Significant speedup:** Typically 5-10x faster for subsequent queries

Steps to use precomputation:

```python
# Initialize your LightRAG instance
rag = LightRAG(
    working_dir="./lightrag_cache",
    embedding_func=your_embedding_function,
    llm_model_func=your_llm_function,
)

# Run precomputation once after loading/indexing your data
await rag.precompute_pagerank_data()

# Now you can run multiple queries with improved performance
results1 = await rag.retrieve_docs_with_pagerank(query="What is machine learning?")
results2 = await rag.retrieve_docs_with_pagerank(query="Explain deep learning")
# ...and so on
```

## How to Use PageRank Retrieval

```python
# Import LightRAG
from lightrag.lightrag import LightRAG

# Initialize your LightRAG instance
rag = LightRAG(
    working_dir="./lightrag_cache",
    embedding_func=your_embedding_function,
    llm_model_func=your_llm_function,
)

# Precompute data for better performance (recommended)
await rag.precompute_pagerank_data()

# Configure PageRank parameters (optional)
pagerank_config = {
    "linking_top_k_facts": 10,       # Number of top facts to consider
    "passage_node_weight_factor": 0.2, # Influence of DPR scores on personalization
    "damping_factor": 0.5,          # Damping factor for PageRank
    "use_synonyms": False,          # Whether to use synonyms
    "direct_dpr_to_chunk_weight": 0.1, # Weight of direct DPR score
    "average_ppr_for_chunk": True,  # Whether to average PPR scores for chunks
}

# Retrieve documents using PageRank algorithm
results = await rag.retrieve_docs_with_pagerank(
    query="What is machine learning?",
    top_k=5,  # Number of documents to retrieve
    pagerank_config=pagerank_config,
    use_precomputed_data=True,  # Use precomputed data for speed
)

# Process results
for result in results:
    print(f"Document ID: {result['id']}, Score: {result['score']}")
    print(f"Content: {result['content'][:100]}...")
```

## Tuning PageRank Parameters

- **linking_top_k_facts**: Controls how many top relations (facts) from the graph to consider. Higher values include more connections but may dilute relevance.

- **passage_node_weight_factor**: Controls how much influence the traditional vector similarity has on the personalization vector. Higher values give more weight to vector similarity.

- **damping_factor**: Controls how far the PageRank algorithm "travels" through the graph. Lower values (e.g. 0.2) allow for more exploration of distant connections, while higher values (e.g. 0.8) stay closer to the personalization biases.

- **direct_dpr_to_chunk_weight**: Controls how much of the original vector similarity score is directly added to the final chunk score. Higher values make the results more similar to traditional vector search.

## Performance Considerations

- **First-time retrieval:** The first retrieval with precomputation will include the time to precompute data
- **Subsequent retrievals:** Will be significantly faster (typically 5-10x)
- **Memory usage:** Precomputation stores embeddings in memory, which increases RAM usage
- **When to precompute:** Call `precompute_pagerank_data()` after you've finished indexing documents and before running queries

For a complete example, see `example_pagerank_retrieval.py`.

Place these three files (`entity_mapping.py`, `test_entity_mapping.py`, `README.md`) in the appropriate location within your project structure. Remember to install `pytest` and `pytest-asyncio` to run the tests.

```

```
